{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/text_52.txt', './data/text_5.txt', './data/text_14.txt', './data/text_33.txt', './data/text_47.txt', './data/text_10.txt', './data/text_42.txt', './data/text_24.txt', './data/text_25.txt', './data/text_0.txt', './data/text_67.txt', './data/text_30.txt', './data/text_29.txt', './data/text_74.txt', './data/text_55.txt', './data/text_9.txt', './data/text_70.txt', './data/text_20.txt', './data/text_59.txt', './data/text_17.txt', './data/text_50.txt', './data/text_54.txt', './data/text_2.txt', './data/text_61.txt', './data/text_3.txt', './data/text_77.txt', './data/text_75.txt', './data/text_1.txt', './data/text_72.txt', './data/text_64.txt', './data/text_53.txt', './data/text_73.txt', './data/text_46.txt', './data/text_32.txt', './data/text_58.txt', './data/text_15.txt', './data/text_49.txt', './data/text_28.txt', './data/text_39.txt', './data/text_68.txt', './data/text_78.txt', './data/text_45.txt', './data/text_62.txt', './data/text_27.txt', './data/text_19.txt', './data/text_36.txt', './data/text_38.txt', './data/text_37.txt', './data/text_48.txt', './data/text_63.txt', './data/text_40.txt', './data/text_23.txt', './data/text_76.txt', './data/text_12.txt', './data/text_31.txt', './data/text_22.txt', './data/text_172.txt', './data/text_21.txt', './data/text_56.txt', './data/text_13.txt', './data/text_66.txt', './data/text_69.txt', './data/text_71.txt', './data/text_43.txt', './data/text_8.txt', './data/text_79.txt', './data/text_18.txt', './data/text_44.txt', './data/text_65.txt', './data/text_51.txt', './data/text_60.txt', './data/text_26.txt', './data/text_35.txt', './data/text_57.txt', './data/text_11.txt', './data/text_34.txt', './data/text_4.txt', './data/text_41.txt', './data/text_7.txt', './data/text_16.txt', './data/text_6.txt']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data\"\n",
    "\n",
    "all_files_path = [os.path.join(data_dir, file_name) for file_name in os.listdir(data_dir)]\n",
    "\n",
    "print(all_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tokenizer.train(\n",
    "    files=all_files_path[:3],\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    limit_alphabet=1000,\n",
    "    wordpieces_prefix='##',\n",
    "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
    "    )\n",
    "\n",
    "models_dir = './models/tokenizer'\n",
    "if not os.path.exists(models_dir): os.mkdir(models_dir)\n",
    "tokenizer.save_model(models_dir, 'bert_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2107: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(f'{models_dir}/bert_tokenizer-vocab.txt', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2319, 2914, 115, 5, 2]\n",
      "['[CLS]', 'surf', '##board', '##ing', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer('surfboarding!')['input_ids']\n",
    "print(token_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
