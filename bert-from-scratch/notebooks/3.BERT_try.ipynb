{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import sentencepiece as spm\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/text_52.txt', '../data/text_5.txt', '../data/text_14.txt', '../data/text_33.txt', '../data/text_47.txt', '../data/text_10.txt', '../data/text_42.txt', '../data/text_24.txt', '../data/text_25.txt', '../data/text_0.txt', '../data/text_67.txt', '../data/text_30.txt', '../data/text_29.txt', '../data/text_74.txt', '../data/text_55.txt', '../data/text_9.txt', '../data/text_70.txt', '../data/text_20.txt', '../data/text_59.txt', '../data/text_17.txt', '../data/text_50.txt', '../data/text_54.txt', '../data/text_2.txt', '../data/text_61.txt', '../data/text_3.txt', '../data/text_77.txt', '../data/text_75.txt', '../data/text_1.txt', '../data/text_72.txt', '../data/text_64.txt', '../data/text_53.txt', '../data/text_73.txt', '../data/text_46.txt', '../data/text_32.txt', '../data/text_58.txt', '../data/text_15.txt', '../data/text_49.txt', '../data/text_28.txt', '../data/text_39.txt', '../data/text_68.txt', '../data/text_78.txt', '../data/text_45.txt', '../data/text_62.txt', '../data/text_27.txt', '../data/text_19.txt', '../data/text_36.txt', '../data/text_38.txt', '../data/text_37.txt', '../data/text_48.txt', '../data/text_63.txt', '../data/text_40.txt', '../data/text_23.txt', '../data/text_76.txt', '../data/text_12.txt', '../data/text_31.txt', '../data/text_22.txt', '../data/text_172.txt', '../data/text_21.txt', '../data/text_56.txt', '../data/text_13.txt', '../data/text_66.txt', '../data/text_69.txt', '../data/text_71.txt', '../data/text_43.txt', '../data/text_8.txt', '../data/text_79.txt', '../data/text_18.txt', '../data/text_44.txt', '../data/text_65.txt', '../data/text_51.txt', '../data/text_60.txt', '../data/text_26.txt', '../data/text_35.txt', '../data/text_57.txt', '../data/text_11.txt', '../data/text_34.txt', '../data/text_4.txt', '../data/text_41.txt', '../data/text_7.txt', '../data/text_16.txt', '../data/text_6.txt']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "\n",
    "all_files_path = [os.path.join(data_dir, file_name) for file_name in os.listdir(data_dir)]\n",
    "\n",
    "print(all_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['only a smattering of bodies filled the rows of chairs set up facing the dais , and most of the folks sitting in them only rested there to fan their feet before launching themselves into the fair again .']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "lines = []\n",
    "with open(all_files_path[0], \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        lines.append(line)\n",
    "\n",
    "sent_tokenize(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2107: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models_dir = '../models/tokenizer'\n",
    "tokenizer = BertTokenizer.from_pretrained(f'{models_dir}/bert_tokenizer-vocab.txt', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_dataset(Dataset):\n",
    "    def __init__(self, files_path, tokenizer, seq_len):\n",
    "        self.files_path = files_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.pairs = []\n",
    "\n",
    "        for i in range(len(files_path)):\n",
    "            lines = []\n",
    "            with open(all_files_path[i], \"r\", encoding=\"utf-8\") as f:\n",
    "                count = 0\n",
    "                for line in f:\n",
    "                    lines.append(line)\n",
    "                    count += 1\n",
    "                    if count == 10000:\n",
    "                        break\n",
    "            \n",
    "            for j in range(len(lines)-1):\n",
    "                self.pairs.append((lines[j], lines[j+1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Step 1: get a random sentence pair, either negative or positive (saved as is_next_label)\n",
    "        #         is_next=1 means the second sentence comes after the first one in the conversation.\n",
    "        s1, s2, is_next = self.get_pair(index)\n",
    "\n",
    "        # Step 2: replace random words in sentence with mask / random words\n",
    "        masked_numericalized_s1, s1_mask = self.mask_sentence(s1)\n",
    "        masked_numericalized_s2, s2_mask = self.mask_sentence(s2)\n",
    "\n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences\n",
    "        # Adding PAD token for labels\n",
    "        t1 = [self.tokenizer.vocab['[CLS]']] + masked_numericalized_s1 + [self.tokenizer.vocab['[SEP]']]\n",
    "        t2 = masked_numericalized_s2 + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_mask = [self.tokenizer.vocab['[PAD]']] + s1_mask + [self.tokenizer.vocab['[PAD]']]\n",
    "        t2_mask = s2_mask + [self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        # Step 4: combine sentence 1 and 2 as one input\n",
    "        # adding PAD tokens to make the sentence same length as seq_len\n",
    "        segment_ids = ([0 for _ in range(len(t1))] + [1 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_mask + t2_mask)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_ids.extend(padding)\n",
    "\n",
    "        # output = {\n",
    "        #     \"bert_input\": bert_input,\n",
    "        #     \"bert_label\": bert_label,\n",
    "        #     \"segment_ids\": segment_ids,\n",
    "        #     \"is_next\": is_next\n",
    "        # }\n",
    "        # return {key: torch.tensor(value) for key, value in output.items()}\n",
    "\n",
    "        return (torch.tensor(bert_input),\n",
    "                torch.tensor(bert_label),\n",
    "                torch.tensor(segment_ids),\n",
    "                torch.tensor(is_next))\n",
    "        \n",
    "    \n",
    "    def get_pair(self, index):\n",
    "        s1, s2 = self.pairs[index]\n",
    "        is_next = 1\n",
    "        if random.random() > 0.5:\n",
    "            random_index = random.randrange(len(self.pairs))\n",
    "            s2 = self.pairs[random_index][1]\n",
    "            is_next = 0\n",
    "        return s1, s2, is_next\n",
    "    \n",
    "    def mask_sentence(self, s):\n",
    "        words = s.split()\n",
    "        masked_numericalized_s = []\n",
    "        mask = []\n",
    "        for word in words:\n",
    "            prob = random.random()\n",
    "            token_ids = self.tokenizer(word)['input_ids'][1:-1]     # remove cls and sep token\n",
    "            if prob < 0.15:                              # Mask out 15% of the words in the input\n",
    "                prob /= 0.15\n",
    "                for token_id in token_ids:                         # Iterate through token ids regardless of masking decision\n",
    "                    if prob < 0.8:                          # Among 15 %, 80% will be replaced with the token 'Mask'\n",
    "                        masked_numericalized_s.append(self.tokenizer.vocab['[MASK]'])\n",
    "                    elif prob < 0.9:                        # Among 15%, 10% will be replaced with a random token\n",
    "                        masked_numericalized_s.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "                    else:                                   # Among 15%, 10% will be left unchanged\n",
    "                        masked_numericalized_s.append(token_id)   # Adding unchanged tokens\n",
    "                    mask.append(token_id)                          # Mask label added for each token\n",
    "            else:\n",
    "                masked_numericalized_s.extend(token_ids)    # Adding tokens directly if not masked\n",
    "                mask.extend([0] * len(token_ids))           # Corresponding unmasked labels\n",
    "\n",
    "        assert len(masked_numericalized_s) == len(mask)\n",
    "        return masked_numericalized_s, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BERT_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mBERT_dataset\u001b[49m(all_files_path[:\u001b[38;5;241m1\u001b[39m], tokenizer, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_input :\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(), result[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BERT_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataset = BERT_dataset(all_files_path[:1], tokenizer, 100)\n",
    "\n",
    "result = test_dataset.__getitem__(1)\n",
    "\n",
    "print(\"bert_input :\", result[0].size(), result[0])\n",
    "print(\"bert_label :\", result[1])\n",
    "print(\"segment_ids :\", result[2])\n",
    "print(\"is_next :\", result[-1])\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(result[0]))\n",
    "print(tokenizer.convert_ids_to_tokens(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.dataloader\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import math\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 970\n",
      "GPU 1: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 175#100\n",
    "\n",
    "n_enc_vocab = tokenizer.vocab_size\n",
    "n_dec_vocab = n_enc_vocab\n",
    "n_output = n_enc_vocab\n",
    "\n",
    "n_enc_seq = seq_len           # json_encode_length\n",
    "n_seg_type = 2\n",
    "n_layers  = 6\n",
    "hid_dim   = 256 # Taille des embeddings\n",
    "pf_dim    = 1024\n",
    "i_pad     = 0\n",
    "n_heads   = 4#8\n",
    "d_head    = 64\n",
    "dropout   = 0.3\n",
    "layer_norm_epsilon = 1e-12\n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "\n",
    "def get_batch(split, batch_size=batch_size, seq_len=seq_len):\n",
    "    all_files_path = [os.path.join(data_dir, file_name) for file_name in os.listdir(data_dir)]\n",
    "    if split == \"train\":\n",
    "        data = BERT_dataset(all_files_path[:1], tokenizer, seq_len)\n",
    "    else:\n",
    "        data = BERT_dataset(all_files_path[1:2], tokenizer, seq_len)\n",
    "\n",
    "    random_index = torch.randint(0, len(data) - batch_size, (batch_size,))\n",
    "\n",
    "    batch = [data[i] for i in random_index]\n",
    "\n",
    "    bert_inputs, bert_labels, segment_ids, is_next = zip(*batch)\n",
    "\n",
    "    bert_inputs = torch.stack(bert_inputs)\n",
    "    bert_labels = torch.stack(bert_labels)\n",
    "    segment_ids = torch.stack(segment_ids)\n",
    "    is_next = torch.stack(is_next)\n",
    "\n",
    "    # TODO : Modification à apport à BERTdataset, pour que ce soit plus performant\n",
    "    # bert_inputs = torch.stack([torch.tensor(data[i:i + seq_len][0]) for i in random_index])\n",
    "    # bert_labels = torch.stack([torch.tensor(data[i + 1:i + 1 + seq_len][1]) for i in random_index])\n",
    "\n",
    "    # if \"caca\" == 'cuda':\n",
    "    #     #x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    #     bert_inputs = bert_inputs.pin_memory().to(device, non_blocking=True)\n",
    "    #     bert_labels = bert_labels.pin_memory().to(device, non_blocking=True)\n",
    "    #     segment_ids = segment_ids.pin_memory().to(device, non_blocking=True)\n",
    "    #     is_next = is_next.pin_memory().to(device, non_blocking=True)\n",
    "    # else:\n",
    "    #     bert_inputs = bert_inputs.to(device)\n",
    "    #     bert_labels = bert_labels.to(device)\n",
    "    #     segment_ids = segment_ids.to(device)\n",
    "    #     is_next = is_next.to(device)\n",
    "\n",
    "    return [bert_inputs, bert_labels, segment_ids, is_next]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "\n",
    "        matmul_qk = torch.matmul(query, torch.transpose(key,2,3))\n",
    "\n",
    "        dk = key.shape[-1]\n",
    "        scaled_attention_logits = matmul_qk / math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        \n",
    "        # Define dense layers corresponding to WQ, WK, and WV\n",
    "        self.query = nn.Linear(hid_dim, n_heads * d_head)\n",
    "        self.key = nn.Linear(hid_dim, n_heads * d_head)\n",
    "        self.value = nn.Linear(hid_dim, n_heads * d_head)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention()\n",
    "        self.dense = nn.Linear(n_heads * d_head, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        # 1. Pass through the dense layer corresponding to WQ\n",
    "        # q : (bs, n_heads, n_q_seq, d_head)\n",
    "        query = self.query(Q).view(batch_size, -1, n_heads, d_head).transpose(1,2)\n",
    "\n",
    "        # 2. Pass through the dense layer corresponding to WK\n",
    "        # k : (bs, n_heads, n_k_seq, d_head)\n",
    "        key   = self.key(K).view(batch_size, -1, n_heads, d_head).transpose(1,2)\n",
    "        \n",
    "        # 3. Pass through the dense layer corresponding to WV\n",
    "        # v : (bs, n_heads, n_v_seq, d_head)\n",
    "        value = self.value(V).view(batch_size, -1, n_heads, d_head).transpose(1,2)\n",
    "\n",
    "        # 4. Scaled Dot Product Attention. Using the previously implemented function\n",
    "        # (bs, n_heads, n_q_seq, n_k_seq)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "\n",
    "        # (bs, n_heads, n_q_seq, d_head), (bs, n_heads, n_q_seq, n_k_seq)\n",
    "        scaled_attention, attn_prob = self.scaled_dot_attn(query, key, value, attn_mask)\n",
    "        \n",
    "        # 5. Concatenate the heads\n",
    "        # (bs, n_heads, n_q_seq, h_head * d_head)\n",
    "        concat_attention = scaled_attention.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_head)\n",
    "        \n",
    "        # 6. Pass through the dense layer corresponding to WO\n",
    "        # (bs, n_heads, n_q_seq, e_embd)\n",
    "        outputs = self.dense(concat_attention)\n",
    "        outputs = self.dropout(outputs)\n",
    "        # (bs, n_q_seq, hid_dim), (bs, n_heads, n_q_seq, n_k_seq)\n",
    "        return outputs, attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionwiseFeedforwardLayer, self).__init__()\n",
    "        self.linear_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.linear_2 = nn.Linear(pf_dim, hid_dim)\n",
    "\n",
    "    def forward(self, attention):\n",
    "        output = self.linear_1(attention)\n",
    "        output = F.relu(output)\n",
    "        output = self.linear_2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttentionLayer()\n",
    "        self.ffn = PositionwiseFeedforwardLayer()\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(hid_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, padding_mask):\n",
    "        \n",
    "        # 1. Encoder mutihead attention is defined\n",
    "        attention, attn_prob = self.attention(inputs, inputs, inputs, padding_mask)\n",
    "        attention   = self.dropout1(attention)\n",
    "        \n",
    "        # 2. 1 st residual layer\n",
    "        attention   = self.layernorm1(inputs + attention)  # (batch_size, input_seq_len, hid_dim)\n",
    "        \n",
    "        # 3. Feed Forward Network\n",
    "        ffn_outputs = self.ffn(attention)  # (batch_size, input_seq_len, hid_dim)\n",
    "        \n",
    "        ffn_outputs = self.dropout2(ffn_outputs)\n",
    "        \n",
    "        # 4. 2 nd residual layer\n",
    "        ffn_outputs = self.layernorm2(attention + ffn_outputs)  # (batch_size, input_seq_len, hid_dim)\n",
    "\n",
    "        # 5. Encoder output of each encoder layer\n",
    "        return ffn_outputs, attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" attention pad mask \"\"\"\n",
    "def create_padding_mask(seq_q, seq_k, i_pad):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)  # <pad>\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(n_enc_vocab, hid_dim)\n",
    "        self.position_embeddings = nn.Embedding(n_enc_seq + 1, hid_dim)\n",
    "        self.token_type_embeddings = nn.Embedding(n_seg_type, hid_dim) #<------------------\n",
    "\n",
    "        self.layer = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        print(\"Encoder :\",inputs.size(), segments.size())\n",
    "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = inputs.eq(i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "\n",
    "        assert torch.all(inputs < self.word_embeddings.num_embeddings), \\\n",
    "            f\"Indices in inputs exceed embedding size: max={inputs.max()}, num_embeddings={self.word_embeddings.num_embeddings}\"\n",
    "        assert torch.all(positions < self.position_embeddings.num_embeddings), \\\n",
    "            f\"Indices in positions exceed embedding size: max={positions.max()}, num_embeddings={self.position_embeddings.num_embeddings}\"\n",
    "        assert torch.all(segments < self.token_type_embeddings.num_embeddings), \\\n",
    "            f\"Indices in segments exceed embedding size: max={segments.max()}, num_embeddings={self.token_type_embeddings.num_embeddings}\"\n",
    "\n",
    "        # (bs, ENCODER_LEN, hid_dim)\n",
    "        outputs = self.word_embeddings(inputs) + self.position_embeddings(positions)  + self.token_type_embeddings(segments)\n",
    "\n",
    "        # (bs, ENCODER_LEN, ENCODER_LEN)\n",
    "        attn_mask = create_padding_mask(inputs, inputs, i_pad)\n",
    "\n",
    "        attn_probs = []\n",
    "        for l in self.layer:\n",
    "            # (bs, ENCODER_LEN, hid_dim), (bs, n_heads, ENCODER_LEN, ENCODER_LEN)\n",
    "            outputs, attn_prob = l(outputs, attn_mask)\n",
    "            attn_probs.append(attn_prob)\n",
    "        # (bs, ENCODER_LEN, hid_dim), [(bs, n_heads, ENCODER_LEN, ENCODER_LEN)]\n",
    "        return outputs, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "\n",
    "        self.linear = nn.Linear(hid_dim, hid_dim)\n",
    "        self.activation = torch.tanh\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        # (bs, n_seq, hid_dim), [(bs, n_heads, n_enc_seq, n_enc_seq)]\n",
    "        outputs, self_attn_probs = self.encoder(inputs, segments)\n",
    "        # (bs, hid_dim)\n",
    "        outputs_cls = outputs[:, 0].contiguous()\n",
    "        outputs_cls = self.linear(outputs_cls)\n",
    "        outputs_cls = self.activation(outputs_cls)\n",
    "        # (bs, n_enc_seq, n_enc_vocab), (bs, hid_dim), [(bs, n_heads, n_enc_seq, n_enc_seq)]\n",
    "        return outputs, outputs_cls, self_attn_probs\n",
    "    \n",
    "    def save(self, epoch, loss, path):\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss,\n",
    "            \"state_dict\": self.state_dict()\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        save = torch.load(path)\n",
    "        self.load_state_dict(save[\"state_dict\"])\n",
    "        return save[\"epoch\"], save[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define Language Model Head \"\"\"\n",
    "class Language_Model_Head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = BERT()\n",
    "        # classfier\n",
    "        self.projection_cls = nn.Linear(hid_dim, 2, bias=False)\n",
    "        # lm\n",
    "        self.projection_lm = nn.Linear(hid_dim, n_output, bias=False)\n",
    "        self.projection_lm.weight = self.bert.encoder.word_embeddings.weight\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        # (bs, n_enc_seq, hid_dim), (bs, hid_dim), [(bs, n_heads, n_enc_seq, n_enc_seq)]\n",
    "        print(\"LM :\", inputs.size(), segments.size())\n",
    "        print(inputs.device.type)\n",
    "        print(segments.device.type)\n",
    "\n",
    "        # assert inputs.device.type == 'cuda'\n",
    "        # assert segments.device.type == 'cuda'\n",
    "\n",
    "        \n",
    "        assert inputs.max() < n_enc_vocab\n",
    "        \n",
    "        \n",
    "        outputs, outputs_cls, attn_probs = self.bert(inputs, segments)\n",
    "        # (bs, 2)\n",
    "        logits_cls = self.projection_cls(outputs_cls)\n",
    "        # (bs, n_enc_seq, n_enc_vocab)\n",
    "        logits_lm = self.projection_lm(outputs)\n",
    "        # (bs, n_enc_vocab), (bs, n_enc_seq, n_enc_vocab), [(bs, n_heads, n_enc_seq, n_enc_seq)]\n",
    "        return logits_cls, logits_lm, attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_M_collate(inputs):\n",
    "    bert_input, bert_label, segment_ids, is_next = list(zip(*inputs))\n",
    "\n",
    "    bert_input  = torch.nn.utils.rnn.pad_sequence(bert_input, batch_first=True, padding_value=0)\n",
    "    bert_label = torch.nn.utils.rnn.pad_sequence(bert_label, batch_first=True, padding_value=0)\n",
    "    segment_ids = torch.nn.utils.rnn.pad_sequence(segment_ids, batch_first=True, padding_value=0)\n",
    "    # src_inputs, trg_outputs  = torch.nn.utils.rnn.pad_sequence([src_inputs,trg_outputs], batch_first=True, padding_value=0)\n",
    "\n",
    "    batch = [\n",
    "        bert_input,\n",
    "        bert_label,\n",
    "        segment_ids,\n",
    "        torch.tensor(is_next)\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L_M_collate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m training_dataset \u001b[38;5;241m=\u001b[39m BERT_dataset(all_files_path[:\u001b[38;5;241m1\u001b[39m], tokenizer, seq_len)\n\u001b[0;32m----> 2\u001b[0m training_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(training_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[43mL_M_collate\u001b[49m)\n\u001b[1;32m      4\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m BERT_dataset(all_files_path[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m], tokenizer, seq_len)\n\u001b[1;32m      5\u001b[0m validation_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(validation_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mL_M_collate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L_M_collate' is not defined"
     ]
    }
   ],
   "source": [
    "training_dataset = BERT_dataset(all_files_path[:1], tokenizer, seq_len)\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, collate_fn=L_M_collate)\n",
    "\n",
    "validation_dataset = BERT_dataset(all_files_path[1:2], tokenizer, seq_len)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, collate_fn=L_M_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_lm contient les scores bruts (non normalisés) pour chaque tokens du vocabulaire, avec l'application softmax on obtient des probabilités de chaque token\n",
    "\n",
    "def custom_loss(logits_cls, logits_lm, bert_label, is_next):\n",
    "    loss_lm = criterion(logits_lm.view(-1, logits_lm.size(-1)), bert_label.view(-1))\n",
    "    loss_nsp = criterion(logits_cls, is_next)\n",
    "\n",
    "    return loss_lm + loss_nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test avec training loader\n",
    "\n",
    "# model = Language_Model_Head()\n",
    "# model.to(device)\n",
    "\n",
    "# bert_input, bert_label, segments_ids, is_next = next(iter(training_loader))\n",
    "# bert_input = bert_input.to(device)\n",
    "# bert_label = bert_label.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model(bert_input, segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test avec get_batch\n",
    "\n",
    "# bert_input_2, bert_label_2, segments_ids_2, is_next_2 = get_batch(\"train\")\n",
    "# model = Language_Model_Head()\n",
    "# model.to(device)\n",
    "\n",
    "# bert_input_2 = bert_input_2.to(device)\n",
    "# bert_label_2 = bert_label_2.to(device)\n",
    "# segments_ids_2 = segments_ids_2.to(device)\n",
    "# is_next_2 = is_next.to(device)\n",
    "\n",
    "# print(bert_input_2.device.type, bert_label_2.device.type, next(model.parameters()).device.type)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model(bert_input_2, segments_ids_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(bar):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(training_loader):\n",
    "\n",
    "        # Retrieve data\n",
    "        bert_input, bert_label, segment_ids, is_next = [data.to(device) for data in batch]\n",
    "\n",
    "        # Zero the gradients for every batchs\n",
    "        optimizer.zero_grad()\n",
    "        print(\"start :\", bert_input.size(), bert_label.size())\n",
    "        # Make predictions for this batch\n",
    "        logits_cls, logits_lm, attn_probs = model(bert_input, segment_ids) # (bs, n_enc_vocab), (bs, n_enc_seq, n_enc_vocab), [(bs, n_heads, n_enc_seq, n_enc_seq)]\n",
    "        \n",
    "        # Compute losse(s)\n",
    "        loss = custom_loss(logits_cls, logits_lm, bert_label, is_next)\n",
    "\n",
    "        # Compute its gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        bar.update(1)\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10\n",
    "            \n",
    "            bar.set_postfix_str(f\"loss: {last_loss:.3f}\")\n",
    "\n",
    "            running_loss = 0\n",
    "        \n",
    "        bar.set_postfix_str(f\"loss: {last_loss:.3f}\")\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "def val_one_epoch(bar):\n",
    "    last_vloss = 0\n",
    "    running_vloss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vbatch in enumerate(validation_loader):\n",
    "            vbert_input, vbert_label, vsegment_ids, vis_next = [vdata.to(device) for vdata in vbatch]\n",
    "\n",
    "            vlogits_cls, vlogits_lm, attn_probs = model(vbert_input, vsegment_ids)\n",
    "\n",
    "            vloss = custom_loss(vlogits_cls, vlogits_lm, vbert_label, vis_next)\n",
    "\n",
    "            running_vloss += vloss\n",
    "            bar.update(1)\n",
    "\n",
    "            if i % 10 == 9:\n",
    "                last_vloss = running_vloss / 10\n",
    "                bar.set_postfix_str(f\"val_loss: {last_vloss:.3f}\")\n",
    "                running_vloss = 0\n",
    "    return last_vloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODELS_PATH = r\"../models/bert-chkpts\"\n",
    "\n",
    "model_files = [\n",
    "    os.path.join(SAVE_MODELS_PATH, f) \n",
    "    for f in os.listdir(SAVE_MODELS_PATH)\n",
    "    if os.path.isfile(os.path.join(SAVE_MODELS_PATH, f))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, ... starting new training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988ae31b78dd48b283ec24529f00635e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/11:   0%|          | 0/500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "start : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "LM : torch.Size([20, 175]) torch.Size([20, 175])\n",
      "cuda\n",
      "cuda\n",
      "Encoder : torch.Size([20, 175]) torch.Size([20, 175])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(training_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_epoch \u001b[38;5;241m+\u001b[39m num_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m train_bar:\n\u001b[1;32m     48\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 49\u001b[0m     avg_tloss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(validation_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m val_bar:\n\u001b[1;32m     52\u001b[0m         model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(bar)\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Gather data and report\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NEW_TRAINING = False\n",
    "DISTRIBUTED = True\n",
    "\n",
    "SAVE_MODELS_PATH = r\"../models/bert-chkpts\"\n",
    "\n",
    "model = Language_Model_Head()\n",
    "# if DISTRIBUTED:\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=i_pad)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "patience = 3\n",
    "\n",
    "start_epoch = 1\n",
    "patience_counter = 0\n",
    "best_vloss = np.inf\n",
    "\n",
    "if not NEW_TRAINING:\n",
    "    try:\n",
    "        model_files = [\n",
    "            os.path.join(SAVE_MODELS_PATH, f) \n",
    "            for f in os.listdir(SAVE_MODELS_PATH) \n",
    "            if os.path.isfile(os.path.join(SAVE_MODELS_PATH, f))\n",
    "        ]\n",
    "\n",
    "        lastest_model_path = max(model_files, key=os.path.getctime)\n",
    "        checkpoint = torch.load(lastest_model_path, weights_only=True)\n",
    "\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        patience_counter = checkpoint[\"patience_counter\"]\n",
    "        best_vloss = checkpoint[\"best_vloss\"]\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"No checkpoint found, ... starting new training\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No checkpoint found, ... starting new training\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs + 1):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    \n",
    "    with tqdm(total=len(training_loader), desc=f\"Epoch {epoch}/{start_epoch + num_epochs}\", unit=\"batch\") as train_bar:\n",
    "        model.train()\n",
    "        avg_tloss = train_one_epoch(train_bar)\n",
    "\n",
    "        with tqdm(total=len(validation_loader), desc=f\"Validation\", unit=\"batch\", leave=False) as val_bar:\n",
    "            model.eval()\n",
    "            avg_vloss = val_one_epoch(val_bar)\n",
    "        train_bar.set_postfix_str(f\"loss: {avg_tloss:.3f} - val_loss: {avg_vloss:.3f}\")\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = f\"bert-small_epoch={epoch}_{timestamp}.pth\"\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"patience_counter\": patience_counter,\n",
    "            \"best_vloss\": best_vloss\n",
    "        }, os.path.join(SAVE_MODELS_PATH, model_path))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.bert.encoder.word_embeddings.weight: torch.Size([30000, 256])\n",
      "module.bert.encoder.position_embeddings.weight: torch.Size([176, 256])\n",
      "module.bert.encoder.token_type_embeddings.weight: torch.Size([2, 256])\n",
      "module.bert.encoder.layer.0.attention.query.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.0.attention.query.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.0.attention.key.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.0.attention.key.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.0.attention.value.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.0.attention.value.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.0.attention.dense.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.0.attention.dense.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.0.ffn.linear_1.weight: torch.Size([1024, 256])\n",
      "module.bert.encoder.layer.0.ffn.linear_1.bias: torch.Size([1024])\n",
      "module.bert.encoder.layer.0.ffn.linear_2.weight: torch.Size([256, 1024])\n",
      "module.bert.encoder.layer.0.ffn.linear_2.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.0.layernorm1.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.0.layernorm1.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.0.layernorm2.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.0.layernorm2.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.1.attention.query.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.1.attention.query.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.1.attention.key.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.1.attention.key.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.1.attention.value.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.1.attention.value.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.1.attention.dense.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.1.attention.dense.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.1.ffn.linear_1.weight: torch.Size([1024, 256])\n",
      "module.bert.encoder.layer.1.ffn.linear_1.bias: torch.Size([1024])\n",
      "module.bert.encoder.layer.1.ffn.linear_2.weight: torch.Size([256, 1024])\n",
      "module.bert.encoder.layer.1.ffn.linear_2.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.1.layernorm1.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.1.layernorm1.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.1.layernorm2.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.1.layernorm2.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.2.attention.query.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.2.attention.query.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.2.attention.key.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.2.attention.key.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.2.attention.value.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.2.attention.value.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.2.attention.dense.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.2.attention.dense.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.2.ffn.linear_1.weight: torch.Size([1024, 256])\n",
      "module.bert.encoder.layer.2.ffn.linear_1.bias: torch.Size([1024])\n",
      "module.bert.encoder.layer.2.ffn.linear_2.weight: torch.Size([256, 1024])\n",
      "module.bert.encoder.layer.2.ffn.linear_2.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.2.layernorm1.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.2.layernorm1.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.2.layernorm2.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.2.layernorm2.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.3.attention.query.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.3.attention.query.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.3.attention.key.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.3.attention.key.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.3.attention.value.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.3.attention.value.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.3.attention.dense.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.3.attention.dense.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.3.ffn.linear_1.weight: torch.Size([1024, 256])\n",
      "module.bert.encoder.layer.3.ffn.linear_1.bias: torch.Size([1024])\n",
      "module.bert.encoder.layer.3.ffn.linear_2.weight: torch.Size([256, 1024])\n",
      "module.bert.encoder.layer.3.ffn.linear_2.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.3.layernorm1.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.3.layernorm1.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.3.layernorm2.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.3.layernorm2.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.4.attention.query.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.4.attention.query.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.4.attention.key.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.4.attention.key.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.4.attention.value.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.4.attention.value.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.4.attention.dense.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.4.attention.dense.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.4.ffn.linear_1.weight: torch.Size([1024, 256])\n",
      "module.bert.encoder.layer.4.ffn.linear_1.bias: torch.Size([1024])\n",
      "module.bert.encoder.layer.4.ffn.linear_2.weight: torch.Size([256, 1024])\n",
      "module.bert.encoder.layer.4.ffn.linear_2.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.4.layernorm1.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.4.layernorm1.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.4.layernorm2.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.4.layernorm2.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.5.attention.query.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.5.attention.query.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.5.attention.key.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.5.attention.key.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.5.attention.value.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.5.attention.value.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.5.attention.dense.weight: torch.Size([256, 256])\n",
      "module.bert.encoder.layer.5.attention.dense.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.5.ffn.linear_1.weight: torch.Size([1024, 256])\n",
      "module.bert.encoder.layer.5.ffn.linear_1.bias: torch.Size([1024])\n",
      "module.bert.encoder.layer.5.ffn.linear_2.weight: torch.Size([256, 1024])\n",
      "module.bert.encoder.layer.5.ffn.linear_2.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.5.layernorm1.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.5.layernorm1.bias: torch.Size([256])\n",
      "module.bert.encoder.layer.5.layernorm2.weight: torch.Size([256])\n",
      "module.bert.encoder.layer.5.layernorm2.bias: torch.Size([256])\n",
      "module.bert.linear.weight: torch.Size([256, 256])\n",
      "module.bert.linear.bias: torch.Size([256])\n",
      "module.projection_cls.weight: torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
